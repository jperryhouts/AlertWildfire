{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "native-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, glob, json, os, random, re, subprocess\n",
    "import numpy as np\n",
    "from scipy import stats, signal\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "%matplotlib inline\n",
    "seaborn.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "guided-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.db_util import get_sql_engine, SQL\n",
    "SQL_ENGINE = get_sql_engine()\n",
    "%load_ext sql\n",
    "%sql $SQL_ENGINE.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-frontier",
   "metadata": {},
   "source": [
    "## Un-archive some data\n",
    "\n",
    "The next cells were used to transfer imagery that I previously captured using the brute-force Selenium scraping method into the same format as the more recent method (direct download). Namely, they are converted to jpg format, a lower-resolution version of each one is stored alongside the full-res one, and the filename time stamp is updated to reflect the time of the image capture, rather than the time of local acquisition.\n",
    "\n",
    "This only needs to happen for old images in the Brightwood dataset. When I moved to the new download format I renamed all of the existing data to prefix with 'Archive' rather than 'AlertWF'."
   ]
  },
  {
   "cell_type": "raw",
   "id": "frank-karma",
   "metadata": {},
   "source": [
    "%time\n",
    "'''\n",
    "We need to fetch actual metadata from the mysql database\n",
    "in order to rename images using their actual timestamp.\n",
    "We'll do that all at once, and dump the results into a json\n",
    "file to be loaded again in the following step. This is\n",
    "mostly just to avoid needing to open the local DB when the\n",
    "meat of this conversion is processed on an ec2 instance.\n",
    "'''\n",
    "\n",
    "selector = SQL.text('SELECT DateTime, Path FROM Metadata;')\n",
    "with SQL_ENGINE.connect().execution_options(autocommit=True) as conn, \\\n",
    "        open('migrate.json','w') as migrate:\n",
    "    res = conn.execute(selector)\n",
    "    keyvals = {key[:-4]: dt.isoformat()+'-08:00' for dt, key in res.fetchall()}\n",
    "    migrate.write(json.dumps(keyvals))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "stuck-integral",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "For migrating images that were scraped using the\n",
    "Selenium brute-force method (screenshotting the actual\n",
    "webpage) into the new format.\n",
    "\n",
    "Note: Run this on an ec2 instance to save on s3 tx fees.\n",
    "\n",
    "scp -i ~/.ssh/piwik_key migrate.json ec2-user@ec2-34-216-82-235.us-west-2.compute.amazonaws.com:\n",
    "'''\n",
    "\n",
    "import boto3, json, os, subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "secrets_path = os.path.join(HOME, 'Documents', 'secrets.json')\n",
    "with open(secrets_path,'r') as secrets_file:\n",
    "    secrets = json.load(secrets_file)\n",
    "    BUCKET_NAME = secrets['aws']['bucket']\n",
    "    PREFIX = secrets['aws']['prefix']\n",
    "\n",
    "TMP_ORIG = '/tmp/original.png'\n",
    "TMP_CONV = '/tmp/converted.jpg'\n",
    "TMP_COMP = '/tmp/compressed.jpg'\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = s3_resource.Bucket(BUCKET_NAME)\n",
    "\n",
    "with open('migrate.json','r') as mfile:\n",
    "    migrate = json.load(mfile)\n",
    "    \n",
    "for obj in bucket.objects.filter(Prefix=PREFIX):\n",
    "    obj_key = obj.key\n",
    "    obj_base = (obj.key.split('/')[-1])[:-4]\n",
    "    if obj_base in migrate:\n",
    "        # Fetch the object to a local temporary file\n",
    "        s3_client.download_file(BUCKET_NAME, obj_key, TMP_ORIG)\n",
    "\n",
    "        # Convert the object to (true) jpg format and compress\n",
    "        subprocess.call(['convert', '-identify', TMP_ORIG, TMP_CONV])\n",
    "        subprocess.call(['convert', '-identify', TMP_ORIG, '-resize', '@250000', TMP_COMP])\n",
    "\n",
    "        # Create new object key\n",
    "        tstamp = migrate[obj_base]\n",
    "        conv_key = f'AlertWF/Brightwood/Brightwood_{tstamp}.jpg'\n",
    "        comp_key = f'AlertWF/Brightwood/Brightwood_{tstamp}-small.jpg'\n",
    "\n",
    "        # Upload converted objects\n",
    "        s3_client.upload_file(TMP_CONV, BUCKET_NAME, conv_key)\n",
    "        s3_client.upload_file(TMP_COMP, BUCKET_NAME, comp_key)\n",
    "\n",
    "        # Delete original object\n",
    "        s3_resource.Object(BUCKET_NAME, obj_key).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-complement",
   "metadata": {},
   "source": [
    "## Now, updating the database"
   ]
  },
  {
   "cell_type": "raw",
   "id": "conscious-commitment",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from tools.alertwf import get_all_cameras\n",
    "\n",
    "engine = create_engine('sqlite:///cameras.db', echo=False)\n",
    "\n",
    "cameras1 = get_all_cameras()\n",
    "cameras1.to_sql('cameras', engine)\n",
    "cameras2 = pd.read_sql('cameras',con=engine)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "funded-greenhouse",
   "metadata": {},
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stations (\n",
    "    id VARCHAR(255),\n",
    "    `name` VARCHAR(255),\n",
    "    `state` VARCHAR(2),\n",
    "    lon FLOAT, lat FLOAT, elevation FLOAT,\n",
    "    PRIMARY KEY (id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS weather (\n",
    "    id INT NOT NULL AUTO_INCREMENT,\n",
    "    station VARCHAR(255),\n",
    "    time_stamp DATETIME,\n",
    "    temp_c FLOAT,\n",
    "    wind_kph FLOAT,\n",
    "    wind_az FLOAT,\n",
    "    precip VARCHAR(255),\n",
    "    PRIMARY KEY (id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS images (\n",
    "    id INT NOT NULL AUTO_INCREMENT,\n",
    "    station VARCHAR(255),\n",
    "    time_stamp DATETIME,\n",
    "    path VARCHAR(255),\n",
    "    res_x INT, res_y INT,\n",
    "    azimuth FLOAT, tilt FLOAT, zoom FLOAT,\n",
    "    night_mode TINYINT,\n",
    "    feature_min FLOAT,\n",
    "    feature_max FLOAT,\n",
    "    feature_mean FLOAT,\n",
    "    feature_median FLOAT,\n",
    "    feature_grad_x_entropy FLOAT,\n",
    "    feature_grad_y_entropy FLOAT,\n",
    "    PRIMARY KEY (id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "serious-acrobat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://jmp:***@172.17.0.3:3306/AlertWildfire\n",
      "0 rows affected.\n",
      "0 rows affected.\n",
      "0 rows affected.\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%sql\n",
    "# ALTER TABLE stations DROP COLUMN elevation;\n",
    "# ALTER TABLE stations ADD elevation_km FLOAT;\n",
    "# ALTER TABLE stations CHANGE name id VARCHAR(255)\n",
    "# ALTER TABLE stations ADD `name` VARCHAR(255);\n",
    "# ALTER TABLE stations ADD `state` VARCHAR(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "distinguished-screen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://jmp:***@172.17.0.3:3306/AlertWildfire\n",
      "2 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>id</th>\n",
       "        <th>lon</th>\n",
       "        <th>lat</th>\n",
       "        <th>elevation_km</th>\n",
       "        <th>name</th>\n",
       "        <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Axis-Aeneas</td>\n",
       "        <td>-119.622</td>\n",
       "        <td>48.7435</td>\n",
       "        <td>1.569</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Axis-AlabamaHills1</td>\n",
       "        <td>-118.09</td>\n",
       "        <td>36.5657</td>\n",
       "        <td>1.383</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('Axis-Aeneas', -119.622, 48.7435, 1.569, None, None),\n",
       " ('Axis-AlabamaHills1', -118.09, 36.5657, 1.383, None, None)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%sql SELECT * FROM stations LIMIT 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "contrary-capture",
   "metadata": {},
   "source": [
    "from tools.alertwf import get_all_cameras\n",
    "cameras = get_all_cameras()\n",
    "\n",
    "check_station = SQL.text(\n",
    "        \"SELECT * FROM stations WHERE `id`=:stationid\"\n",
    "    ).bindparams(SQL.bindparam(\"stationid\", type_=SQL.String))\n",
    "\n",
    "insert_station_query = SQL.text(\n",
    "        \"INSERT INTO stations \" +\n",
    "        \"(`id`, `name`, lon, lat, elevation_km, `state`) \" +\n",
    "        \"VALUES (:stationid, :name, :lon, :lat, :elevation_km, :state);\"\n",
    "    ).bindparams(\n",
    "        SQL.bindparam(\"stationid\", type_=SQL.String),\n",
    "        SQL.bindparam(\"name\", type_=SQL.String),\n",
    "        SQL.bindparam(\"lon\", type_=SQL.Float),\n",
    "        SQL.bindparam(\"lat\", type_=SQL.Float),\n",
    "        SQL.bindparam(\"elevation_km\", type_=SQL.Float),\n",
    "        SQL.bindparam(\"state\", type_=SQL.String)\n",
    "    )\n",
    "\n",
    "update_station_query = SQL.text(\n",
    "        \"UPDATE stations SET `name`=:name, `state`=:state \" +\n",
    "        \"WHERE `id`=:stationid;\"\n",
    "    ).bindparams(\n",
    "        SQL.bindparam(\"stationid\", type_=SQL.String),\n",
    "        SQL.bindparam(\"name\", type_=SQL.String),\n",
    "        SQL.bindparam(\"state\", type_=SQL.String)\n",
    "    )\n",
    "\n",
    "\n",
    "with SQL_ENGINE.connect().execution_options(autocommit=True) as conn:\n",
    "    for station in cameras.index:\n",
    "        res = conn.execute(check_station, {'stationid':station})\n",
    "        row = cameras.loc[station]\n",
    "        params = dict(stationid=station, \\\n",
    "                      name=row['name'],\n",
    "                      lon=row['longitude'], \\\n",
    "                      lat=row['latitude'], \\\n",
    "                      elevation_km=row['elevation_km'], \\\n",
    "                      state=row['state'])\n",
    "        if len(res.fetchall()) == 0:\n",
    "            conn.execute(insert_station_query, params)\n",
    "        else:\n",
    "            conn.execute(update_station_query, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "appropriate-stone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 37835 records\n",
      "CPU times: user 1min 56s, sys: 1.85 s, total: 1min 57s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "params = dict(stationid=SQL.String, time_stamp=SQL.DateTime, path=SQL.String)\n",
    "\n",
    "insert_image_query = SQL.text('''\n",
    "        INSERT INTO images (`station`, `time_stamp`, `path`)\n",
    "        VALUES (:stationid, :time_stamp, :path);\n",
    "    ''').bindparams(*[SQL.bindparam(p, type_=t) for p,t in params.items()])\n",
    "\n",
    "import boto3, os, json, re\n",
    "from datetime import datetime\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "secrets_path = os.path.join(HOME, 'Documents', 'secrets.json')\n",
    "with open(secrets_path,'r') as secrets_file:\n",
    "    secrets = json.load(secrets_file)\n",
    "    BUCKET_NAME = secrets['aws']['bucket']\n",
    "    PREFIX = secrets['aws']['prefix']\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket = s3_resource.Bucket(BUCKET_NAME)\n",
    "\n",
    "regex_path = re.compile(r'([^/]*)/([^/]*.jpg)$')\n",
    "regex_date = re.compile(r'(\\d\\d\\d\\d-\\d\\d-\\d\\dT\\d\\d:\\d\\d:\\d\\d-\\d\\d:\\d\\d)')\n",
    "\n",
    "def func():\n",
    "    n=0\n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        img_res = conn.execute('SELECT path FROM images')\n",
    "        img_paths = [record[0] for record in img_res.fetchall()]\n",
    "\n",
    "        tx = conn.begin()\n",
    "        for obj in bucket.objects.filter(Prefix=PREFIX):\n",
    "            if ('small' in obj.key) or (obj.key in img_paths):\n",
    "                continue\n",
    "            match = regex_path.search(obj.key)\n",
    "            if match:\n",
    "                station, fname = match.groups()\n",
    "                stamp = regex_date.search(fname).groups()[0]\n",
    "\n",
    "                dtstamp = datetime.fromisoformat(stamp)\n",
    "                stationid = f'Axis-{station}'\n",
    "\n",
    "                row = dict(stationid=stationid, time_stamp=dtstamp, path=obj.key)\n",
    "                conn.execute(insert_image_query, row)\n",
    "\n",
    "                n += 1\n",
    "        tx.commit()\n",
    "    print('Inserted', n, 'records')\n",
    "\n",
    "%time func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-gathering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
