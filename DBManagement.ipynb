{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coral-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, glob, json, os, random, re, subprocess\n",
    "import numpy as np\n",
    "from scipy import stats, signal\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import pymysql\n",
    "import sqlalchemy as SQL\n",
    "from urllib.parse import quote_plus as QP\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "%matplotlib inline\n",
    "seaborn.set_theme()\n",
    "\n",
    "HOME = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hollywood-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_url():\n",
    "    sql_secrets_path = os.path.join(HOME, 'Documents', 'sql_secrets.json')\n",
    "    with open(sql_secrets_path,'r') as SECRETS:\n",
    "        SECRETS = json.load(SECRETS)\n",
    "        SQL_PASSWD = SECRETS['PASSWD']\n",
    "        SQL_USER = SECRETS['USER']\n",
    "        SQL_HOST = SECRETS['HOST']\n",
    "        SQL_PORT = SECRETS['PORT']\n",
    "        SQL_DB = SECRETS['DB']\n",
    "\n",
    "    return f\"mysql+pymysql://{SQL_USER}:{QP(SQL_PASSWD)}@{SQL_HOST}:{SQL_PORT}/{SQL_DB}\"\n",
    "\n",
    "SQL_URL = get_sql_url()\n",
    "SQL_ENGINE = SQL.create_engine(SQL_URL)\n",
    "%load_ext sql\n",
    "%sql $SQL_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-breast",
   "metadata": {},
   "source": [
    "## Un-archive some data\n",
    "\n",
    "The next cells were used to transfer imagery that I previously captured using the brute-force Selenium scraping method into the same format as the more recent method (direct download). Namely, they are converted to jpg format, a lower-resolution version of each one is stored alongside the full-res one, and the filename time stamp is updated to reflect the time of the image capture, rather than the time of local acquisition.\n",
    "\n",
    "This only needs to happen for old images in the Brightwood dataset. When I moved to the new download format I renamed all of the existing data to prefix with 'Archive' rather than 'AlertWF'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "'''\n",
    "We need to fetch actual metadata from the mysql database\n",
    "in order to rename images using their actual timestamp.\n",
    "We'll do that all at once, and dump the results into a json\n",
    "file to be loaded again in the following step. This is\n",
    "mostly just to avoid needing to open the local DB when the\n",
    "meat of this conversion is processed on an ec2 instance.\n",
    "'''\n",
    "\n",
    "selector = SQL.text('SELECT DateTime, Path FROM Metadata;')\n",
    "with SQL_ENGINE.connect().execution_options(autocommit=True) as conn, \\\n",
    "        open('migrate.json','w') as migrate:\n",
    "    res = conn.execute(selector)\n",
    "    keyvals = {key[:-4]: dt.isoformat()+'-08:00' for dt, key in res.fetchall()}\n",
    "    migrate.write(json.dumps(keyvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "For migrating images that were scraped using the\n",
    "Selenium brute-force method (screenshotting the actual\n",
    "webpage) into the new format.\n",
    "\n",
    "Note: Run this on an ec2 instance to save on s3 tx fees.\n",
    "\n",
    "scp -i ~/.ssh/piwik_key migrate.json ec2-user@ec2-34-216-82-235.us-west-2.compute.amazonaws.com:\n",
    "'''\n",
    "\n",
    "import boto3, json, subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "BUCKET_NAME = 'storage-9iudgkuqwurq6'\n",
    "PREFIX = 'Archive/Brightwood'\n",
    "\n",
    "TMP_ORIG = '/tmp/original.png'\n",
    "TMP_CONV = '/tmp/converted.jpg'\n",
    "TMP_COMP = '/tmp/compressed.jpg'\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = s3_resource.Bucket(BUCKET_NAME)\n",
    "\n",
    "with open('migrate.json','r') as mfile:\n",
    "    migrate = json.load(mfile)\n",
    "    \n",
    "for obj in bucket.objects.filter(Prefix=PREFIX):\n",
    "    obj_key = obj.key\n",
    "    obj_base = (obj.key.split('/')[-1])[:-4]\n",
    "    if obj_base in migrate:\n",
    "        # Fetch the object to a local temporary file\n",
    "        s3_client.download_file(BUCKET_NAME, obj_key, TMP_ORIG)\n",
    "\n",
    "        # Convert the object to (true) jpg format and compress\n",
    "        subprocess.call(['convert', '-identify', TMP_ORIG, TMP_CONV])\n",
    "        subprocess.call(['convert', '-identify', TMP_ORIG, '-resize', '@250000', TMP_COMP])\n",
    "\n",
    "        # Create new object key\n",
    "        tstamp = migrate[obj_base]\n",
    "        conv_key = f'AlertWF/Brightwood/Brightwood_{tstamp}.jpg'\n",
    "        comp_key = f'AlertWF/Brightwood/Brightwood_{tstamp}-small.jpg'\n",
    "\n",
    "        # Upload converted objects\n",
    "        s3_client.upload_file(TMP_CONV, BUCKET_NAME, conv_key)\n",
    "        s3_client.upload_file(TMP_COMP, BUCKET_NAME, comp_key)\n",
    "\n",
    "        # Delete original object\n",
    "        s3_resource.Object(BUCKET_NAME, obj_key).delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
