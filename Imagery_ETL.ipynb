{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "motivated-parish",
   "metadata": {},
   "source": [
    "# Extract metadata from AlertWildfire imagery\n",
    "\n",
    "This script loops over images scraped from the AlertWildfire website, extracting metadata from the embedded watermark. It adds a record to the `AlertWildfire.Metadata` table for each image, containing information about which station it's from, its orientation (azimuth/tilt), and its date/time stamp.\n",
    "\n",
    "> Note: Scraping is done by the WebScraper/scrape.py script, which dumps raw images into an s3 bucket.\n",
    "> This script operates on those images by mounting the bucket to a local directory with s3fs.\n",
    "> A more efficient method would be scraping images to the local filesystem, then moving them to s3 after they have been parsed for metadata.\n",
    "> This works fine for one image stream, and if I were to scale this pipeline I would want to optimize this process for automation anyway.\n",
    "\n",
    "### First: import packages, and open connection to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floppy-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, json, os, random, re\n",
    "import numpy as np\n",
    "from scipy import stats, signal\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import pymysql\n",
    "import sqlalchemy as SQL\n",
    "from urllib.parse import quote_plus as QP\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "%matplotlib inline\n",
    "seaborn.set_theme()\n",
    "\n",
    "HOME = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "processed-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_url():\n",
    "    sql_secrets_path = os.path.join(HOME, 'Documents', 'sql_secrets.json')\n",
    "    with open(sql_secrets_path,'r') as SECRETS:\n",
    "        SECRETS = json.load(SECRETS)\n",
    "        SQL_PASSWD = SECRETS['PASSWD']\n",
    "        SQL_USER = SECRETS['USER']\n",
    "        SQL_HOST = SECRETS['HOST']\n",
    "        SQL_PORT = SECRETS['PORT']\n",
    "        SQL_DB = SECRETS['DB']\n",
    "\n",
    "    return f\"mysql+pymysql://{SQL_USER}:{QP(SQL_PASSWD)}@{SQL_HOST}:{SQL_PORT}/{SQL_DB}\"\n",
    "\n",
    "SQL_URL = get_sql_url()\n",
    "#print(SQL_URL)\n",
    "SQL_ENGINE = SQL.create_engine(SQL_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expensive-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql $SQL_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-woman",
   "metadata": {},
   "source": [
    "### Create image processing pipeline\n",
    "\n",
    "We're going to need to extract metadata from images using OCR, so let's set up some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comprehensive-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watermark_region(img):\n",
    "    '''\n",
    "    Crop full image down to just the black/white rectangle in the lower left.\n",
    "    Returns a cropped grayscale image ~26 pixels high.\n",
    "    '''\n",
    "    # Clip out the watermark line (full img width)\n",
    "    W, H = img.size\n",
    "    row_segment = img.crop((0, H-28, W, H-2))\n",
    "    \n",
    "    # Given a wide image segment with white text over black background on the\n",
    "    # left side, crop out anything beyond the right edge of the black box.\n",
    "    pix = np.asarray(row_segment)\n",
    "    mode = stats.mode(pix, axis=0).mode[0]\n",
    "    M = 1.0 * ((mode > 5) & (mode < 200))\n",
    "    win = np.zeros(M.shape)\n",
    "    win[win.size//2-15:win.size//2+15] = 1\n",
    "    c = signal.convolve(M, win/win.sum(), mode='same')\n",
    "    c = 1.0 * (c > 0.5)\n",
    "    c[-10:] = 1.0\n",
    "    N = np.where(c == 1.0)[0].min()\n",
    "    row_h = row_segment.size[1]\n",
    "\n",
    "    watermark = row_segment.crop((0, 0, N, row_h))\n",
    "    return watermark\n",
    "\n",
    "def extract_metadata(fname):\n",
    "    '''\n",
    "    Wrapper function to process a single image. Returns a dictionary with\n",
    "    the following values:\n",
    "    {\n",
    "      station: str,\n",
    "      timestamp: datetime,\n",
    "      loc: tuple(double, double double)\n",
    "    }\n",
    "    '''\n",
    "    with Image.open(fname) as img:\n",
    "        # Convert to grayscale\n",
    "        img = img.convert('L')\n",
    "        \n",
    "        # Extract the watermark segment of the image\n",
    "        watermark = get_watermark_region(img)\n",
    "\n",
    "        # Read the watermark text using OCR\n",
    "        stamp = pytesseract.image_to_string(watermark)\n",
    "        stamp = stamp.strip().replace(' ', '').lower()\n",
    "\n",
    "        ret = dict()\n",
    "        \n",
    "        # Extract data fields from the watermark text\n",
    "        stmatch = re.search(r'axis-(.*)\\s*x:', stamp)\n",
    "        if stmatch:\n",
    "            ret['station'] = stmatch.groups()[0].title()\n",
    "        stmatch = re.search(r'axis-(.*)\\s*elev', stamp)\n",
    "        if stmatch:\n",
    "            ret['station'] = stmatch.groups()[0].title()\n",
    "        \n",
    "        xyzmatch = re.search(r'x:(.*)y:(.*)z:([0-9\\.-]*).*or', stamp)\n",
    "        if xyzmatch:\n",
    "            X, Y, Z = [((float(v)+1.0)-1.0) for v in xyzmatch.groups()]\n",
    "            ret['loc'] = (X, Y, Z)\n",
    "        \n",
    "        dtmatch = re.search(r'(\\d\\d\\d\\d/\\d\\d/\\d\\d\\d\\d:\\d\\d:\\d\\d)', stamp)\n",
    "        if dtmatch:\n",
    "            dtstr = dtmatch.groups()[0]\n",
    "            ret['timestamp'] = datetime.strptime(dtstr, '%Y/%m/%d%H:%M:%S')\n",
    "        else:\n",
    "            dtmatch = re.search(r'(\\d\\d\\d\\d-\\d\\d-\\d\\dT\\d\\d:\\d\\d:\\d\\d)', fname)\n",
    "            if dtmatch:\n",
    "                dtstr = dtmatch.groups()[0]\n",
    "                ret['timestamp'] = datetime.strptime(dtstr, '%Y-%m-%dT%H:%M:%S')\n",
    "        \n",
    "        if all([(key in ret) for key in ('station', 'loc', 'timestamp')]):\n",
    "            return ret\n",
    "        else:\n",
    "            plt.imshow(watermark, cmap=plt.cm.gray)\n",
    "            plt.show()\n",
    "            return stamp"
   ]
  },
  {
   "cell_type": "raw",
   "id": "technical-response",
   "metadata": {},
   "source": [
    "## DEBUGGING THE PIPELINE\n",
    "## Test metadata extraction on random selection of stored images\n",
    "\n",
    "img_paths_glob = os.path.join(HOME, 'Data', 'Storage', 'AlertWF', '*', '*.png')\n",
    "all_img_paths = glob.glob(img_paths_glob)\n",
    "\n",
    "random.shuffle(all_img_paths)\n",
    "N_SAMPLES = 100\n",
    "N_FAILED = 0\n",
    "for i, fname in enumerate(all_img_paths[:N_SAMPLES]):\n",
    "    mdata = extract_metadata(fname)        \n",
    "    if type(mdata) is str:\n",
    "        N_FAILED += 1\n",
    "        print((\"Unable to parse metadata from image: \"+basename).ljust(100))\n",
    "        print(mdata)\n",
    "    else:\n",
    "        COMPLETED = '%5.01f%%'%(i/N_SAMPLES*100)\n",
    "        STATUS = f\"{COMPLETED} {os.path.basename(fname)}\"\n",
    "        print(STATUS.ljust(100), end='\\x1b[2K\\r')\n",
    "\n",
    "print((\"%5.01f%% SUCCESS\"%((N_SAMPLES-N_FAILED)/N_SAMPLES*100)).ljust(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-license",
   "metadata": {},
   "source": [
    "### Now, define functions for processing images from the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adjacent-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define some SQL queries to use when processing images.\n",
    "'''\n",
    "insert = SQL.text(\n",
    "        \"INSERT INTO Metadata \" +\n",
    "        \"(StationID, DateTime, X, Y, Z, Path) \" +\n",
    "        \"VALUES (:stationid, :timestamp, :x, :y, :z, :path);\"\n",
    "    ).bindparams(\n",
    "        SQL.bindparam(\"stationid\", type_=SQL.Integer),\n",
    "        SQL.bindparam(\"timestamp\", type_=SQL.DateTime),\n",
    "        SQL.bindparam(\"x\", type_=SQL.Float),\n",
    "        SQL.bindparam(\"y\", type_=SQL.Float),\n",
    "        SQL.bindparam(\"z\", type_=SQL.Float),\n",
    "        SQL.bindparam(\"path\", type_=SQL.String)\n",
    "    )\n",
    "\n",
    "def query_existing_records():\n",
    "    '''\n",
    "    Cache all station ID's for faster processing later (avoids needing to make\n",
    "    separate queries to the `Stations` table for each image).\n",
    "    '''\n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        station_records = conn.execute(SQL.text(\"SELECT * FROM Stations;\"))\n",
    "        stations = {st[1]: st[0] for st in station_records}\n",
    "\n",
    "        image_path_records = conn.execute(SQL.text(\"SELECT Path from Metadata;\"))\n",
    "        db_image_paths = [r[0] for r in image_path_records]\n",
    "\n",
    "    return (stations, db_image_paths)\n",
    "\n",
    "def process_images(data_dir, stations, db_image_paths):\n",
    "    '''\n",
    "    Now finally, loop through all images in the data path, parse\n",
    "    each one's metadata, then add it to the Metadata table.\n",
    "    '''\n",
    "    with SQL_ENGINE.connect().execution_options(autocommit=True) as conn:\n",
    "        img_paths_glob = os.path.join(data_dir, '*', '*.png')\n",
    "\n",
    "        all_img_paths = glob.glob(img_paths_glob)\n",
    "        new_img_paths = [P for P in all_img_paths if not os.path.basename(P) in db_image_paths]\n",
    "\n",
    "        N_TOTAL = len(all_img_paths)\n",
    "        N_NEW = len(new_img_paths)\n",
    "        N_DUP = N_TOTAL-N_NEW\n",
    "        N_ADDED, N_PROCESSED, N_ERR = (0, 0, 0)\n",
    "        for fname in sorted(new_img_paths):\n",
    "            N_PROCESSED += 1\n",
    "            COMPLETED = '%5.01f%%'%(N_PROCESSED/N_NEW*100)\n",
    "            print(COMPLETED, end='\\r')\n",
    "\n",
    "            basename = os.path.basename(fname)\n",
    "\n",
    "            #Extract metadata from image\n",
    "            mdata = extract_metadata(fname)        \n",
    "\n",
    "            if mdata is None or type(mdata) is str:\n",
    "                N_ERR += 1\n",
    "                print((\"Unable to parse metadata from image: \"+basename).ljust(100))\n",
    "                print(mdata)\n",
    "                continue\n",
    "\n",
    "            if not mdata['station'] in stations.keys():\n",
    "                N_ERR += 1\n",
    "                print((\"Cannot find station: \"+mdata['station']).ljust(100))\n",
    "                continue\n",
    "\n",
    "            # Add record\n",
    "            X, Y, Z = mdata['loc']\n",
    "            datarow = dict(stationid=stations[mdata['station']],\n",
    "                           timestamp=mdata['timestamp'],\n",
    "                           x=X, y=Y, z=Z, path=basename)\n",
    "\n",
    "            conn.execute(insert, datarow)\n",
    "            N_ADDED += 1\n",
    "\n",
    "            # Print status update\n",
    "            STATUS = f\"{COMPLETED} {mdata['timestamp']}, {mdata['loc']}, {basename}\"\n",
    "            print(STATUS.ljust(100), end='\\r')\n",
    "\n",
    "    print(f\"\\n\\n\\tAdded {N_ADDED} records\")\n",
    "    print(f\"\\tSkipped {N_DUP} existing records\")\n",
    "    print(f\"\\tUnable to process {N_ERR} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-electric",
   "metadata": {},
   "source": [
    "### Bringing it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cleared-large",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% 2021-03-13 11:19:58, (-120.0, -3.0, 2.0), Brightwood_2021-03-13T11:20:18.685063.png          png\n",
      "\n",
      "\tAdded 3424 records\n",
      "\tSkipped 28664 existing records\n",
      "\tUnable to process 0 images\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(HOME, 'Data', 'Storage', 'AlertWF')\n",
    "\n",
    "stations, db_image_paths = query_existing_records()\n",
    "process_images(DATA_DIR, stations, db_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "occupied-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vaex\n",
    "# with open('/home/jmp/foobar.csv', 'w') as csvfile:\n",
    "#     csvfile.write('\"foo\",\"bar\",\"baz\"\\n')\n",
    "#     csvfile.write('1,2,3\\n')\n",
    "#     csvfile.write('4,5,6\\n')\n",
    "#     csvfile.write('7,8,9\\n')\n",
    "\n",
    "# vcsv = vaex.from_csv('/home/jmp/foobar.csv', chunk_size=5_000_000)\n",
    "# for i, d in enumerate(vcsv):\n",
    "#     fname = f\"/home/jmp/Documents/AlertWildfire/foobar{i}.hdf5\"\n",
    "#     if os.path.exists(fname):\n",
    "#         os.remove(fname)\n",
    "#     d.export_hdf5(fname)\n",
    "\n",
    "# dv = vaex.open(\"/home/jmp/Documents/AlertWildfire/foobar0.hdf5\")\n",
    "# print(dv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
